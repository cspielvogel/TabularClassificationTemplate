[Display]
VERBOSE = True

[Data]
# Data file paths can be supplied as relative or absolute path
INPUT_DATA_FILE_PATH = Data/test_data.csv
INPUT_DATA_FILE_SEPARATOR = ;
OUTPUT_DATA_FOLDER_PATH = Output
LABEL_COLUMN_NAME = 1

[EDA]
PERFORM_EDA = False

[Calibration]
PERFORM_CALIBRATION = True

[Training]
NUMBER_OF_FOLDS = 50
RANDOMIZEDSEARCHCV_CV = 10
RANDOMIZEDSEARCHCV_N_ITER = 25

[Classifiers]
# Classifier names must be separated by a coma and subsequent space
CLASSIFIERS_TO_RUN = ebm

[EBM_hyperparameters]
MAX_BINS = 256
MAX_INTERACTION_BINS = 64
BINNING = quantile
INTERACTIONS = 15
OUTER_BAGS = 8, 16
INNER_BAGS = 0, 8
LEARNING_RATE = 0.001, 0.01
EARLY_STOPPING_ROUNDS = 50
EARLY_STOPPING_TOLERANCE = 0.0001
MAX_ROUNDS = 7500
MIN_SAMPLES_LEAF = 2, 4
MAX_LEAVES = 3
N_JOBS = -2

[KNN_hyperparameters]
WEIGHTS = distance
N_NEIGHBORS = adaptive
P = 1, 2, 3, 4, 5

[DT_hyperparameters]
SPLITTER = best, random
MAX_DEPTH = 1, 2, 3, 4, 5, 7, 10, 15
MIN_SAMPLES_SPLIT = 2, 4, 6
MIN_SAMPLES_LEAF = 1, 3, 5, 6
MAX_FEATURES = auto, sqrt, log2

[NN_hyperparameters]
# Hidden layer sizes must be supplied as python tuples separated with a coma e.g. (4, 16, 4), (8, 32, 8)
HIDDEN_LAYER_SIZES = (32, 64, 32)
EARLY_STOPPING = True
N_ITER_NO_CHANGE = 20
MAX_ITER = 1000
ACTIVATION = relu, tanh, logistic
SOLVER = adam
LEARNING_RATE_INIT = 0.01, 0.001, 0.0001

[RF_hyperparameters]
CRITERION = entropy
N_ESTIMATORS = 500
MAX_DEPTH = 1, 2, 3, 4, 5, 7, 10, 15
MIN_SAMPLES_SPLIT = 2, 4, 6
MIN_SAMPLES_LEAF = 1, 3, 5, 6
MAX_FEATURES = auto, sqrt, log2

[XGB_hyperparameters]
LEARNING_RATE = 0.2, 0.3
MAX_DEPTH = 2, 4, 6, 8
MIN_CHILD_WEIGHT = 1, 3
GAMMA = 0, 0.2
COLSAMPLE_BYTREE = 0.5, 0.7, 1.0
